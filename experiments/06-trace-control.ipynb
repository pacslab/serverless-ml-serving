{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4f7b070ef049bbd8502237a868479304b4da16a010bb509c19371ab8ff01cc23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import time\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "# library imports\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pacswg.timer import TimerClass\n",
    "import pacswg\n",
    "\n",
    "from exp_trace_utils import get_time_with_tz\n",
    "import exp_trace_utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# configs\n",
    "server_address = 'http://localhost:3000'\n",
    "\n",
    "# service_name = 'tfserving-resnetv2'\n",
    "# slo_timeout = \n",
    "\n",
    "# service_name = 'bentoml-onnx-resnet50'\n",
    "# slo_timeout = \n",
    "\n",
    "service_name = 'bentoml-iris'\n",
    "service_url = f'{server_address}/proxy/{service_name}'\n",
    "slo_timeout = 200\n",
    "initial_batch_size = 5\n",
    "bs_config = {\n",
    "    'max_bs': 20,\n",
    "    'min_bs': 1,\n",
    "    'inc_step': 5,\n",
    "    'dec_mult': 0.7,\n",
    "}\n",
    "average_timeout_ratio_threshold = 0.9\n",
    "max_rps = 150\n",
    "\n",
    "# service_name = 'tfserving-mobilenetv1'\n",
    "# slo_timeout = \n",
    "\n",
    "# service_name = 'bentoml-keras-toxic-comments'\n",
    "# service_url = f'{server_address}/proxy/{service_name}'\n",
    "# slo_timeout = 500\n",
    "# initial_batch_size = 5\n",
    "# bs_config = {\n",
    "#     'max_bs': 20,\n",
    "#     'min_bs': 1,\n",
    "#     'inc_step': 5,\n",
    "#     'dec_mult': 0.7,\n",
    "# }\n",
    "# average_timeout_ratio_threshold = 0.5\n",
    "# max_rps = 50\n",
    "\n",
    "# service_name = 'bentoml-pytorch-fashion-mnist'\n",
    "# service_url = f'{server_address}/proxy/{service_name}'\n",
    "# slo_timeout = 1000\n",
    "# initial_batch_size = 5\n",
    "# bs_config = {\n",
    "#     'max_bs': 20,\n",
    "#     'min_bs': 1,\n",
    "#     'inc_step': 5,\n",
    "#     'dec_mult': 0.7,\n",
    "# }\n",
    "# average_timeout_ratio_threshold = 0.5\n",
    "# max_rps = 30\n",
    "\n",
    "# SLO Target\n",
    "slo_target = slo_timeout * 0.8\n",
    "\n",
    "# length of measurements used to estimate different batch size response times\n",
    "upstream_rt_max_len = 1000\n",
    "\n",
    "# disable controller?\n",
    "disable_controller = False\n",
    "if disable_controller:\n",
    "    initial_batch_size = 1\n",
    "\n",
    "# experiment info\n",
    "cpu_m = 1000\n",
    "ram_mb = 1024\n",
    "\n",
    "controller = exp_trace_utils.SmartProxyController(\n",
    "    server_address=server_address,\n",
    "    service_name=service_name,\n",
    "    slo_timeout=slo_timeout,\n",
    "    initial_batch_size=initial_batch_size,\n",
    "    bs_config=bs_config,\n",
    "    average_timeout_ratio_threshold=average_timeout_ratio_threshold,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# initialize trace values\n",
    "\n",
    "# staircase workload\n",
    "# base_rps = 5\n",
    "# rps_list = [base_rps] * 5 + [base_rps*2] * 5 + [base_rps*3] * 5 + [base_rps*4] * 5\n",
    "# trace_name = 'staircase'\n",
    "\n",
    "# autoscale traces\n",
    "trace_name = 'trace_wc'\n",
    "autoscale_folder_path = '../traces/files/AutoScale/'\n",
    "autoscale_trace_file_names = {\n",
    "  # Big Spike, NLANR [nlanr1995]\n",
    "  'trace_t2': 'trace_t2.txt',\n",
    "  # Dual Phase, NLANR [nlanr1995]\n",
    "  'trace_t4': 'trace_t4.txt',\n",
    "  # Large variations, NLANR [nlanr1995]\n",
    "  'trace_t5': 'trace_t5.txt',\n",
    "  # worldcup, slowly varying [ita 1998]\n",
    "  'trace_wc': 'trace_wc.txt',\n",
    "}\n",
    "autoscale_file_path = autoscale_folder_path + autoscale_trace_file_names[trace_name]\n",
    "print('loading file:', autoscale_file_path)\n",
    "trace_arr = np.loadtxt(autoscale_file_path)\n",
    "rps_list = trace_arr / (trace_arr.max()) * max_rps\n",
    "rps_list = list(rps_list)\n",
    "\n",
    "plt.plot(rps_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading file: ../traces/files/AutoScale/trace_wc.txt\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff58a1b9580>]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting and Setting Stats and Configs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "controller.set_initial_config()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'serviceName': 'bentoml-iris',\n",
       " 'upstreamUrl': 'http://bentoml-iris.default.192-168-23-125.nip.io/predict',\n",
       " 'maxBufferTimeoutMs': 160,\n",
       " 'maxBufferSize': 5,\n",
       " 'isTFServing': False}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "proxy_stats = controller.get_proxy_stats()\n",
    "proxy_stats"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'maxBufferSize': 5,\n",
       " 'averageMaxBufferSize': 1.4,\n",
       " 'averageActualBatchSize': None,\n",
       " 'maxBufferTimeoutMs': 160,\n",
       " 'currentReplicaCount': 0,\n",
       " 'currentReadyReplicaCount': 0,\n",
       " 'currentConcurrency': 0,\n",
       " 'averageConcurrency': 0,\n",
       " 'averageArrivalRate': 0,\n",
       " 'averageDepartureRate': 0,\n",
       " 'averageDispatchRate': 0,\n",
       " 'averageErrorRate': 0,\n",
       " 'averageTimeoutRatio': None,\n",
       " 'reponseTimeAverage': None,\n",
       " 'reponseTimeP50': None,\n",
       " 'reponseTimeP95': None,\n",
       " 'batchResponseTimeStats': {}}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "batch_rt_values = controller.update_batch_rt_values()\n",
    "# batch_rt_values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# run a loop to monitor response times to test the functionality\n",
    "# timer = TimerClass()\n",
    "\n",
    "# batch_rt_values = {}\n",
    "# for _ in tqdm(range(1*6)):\n",
    "#     timer.tic()\n",
    "#     controller.update_batch_rt_values()\n",
    "#     while timer.toc() < 10:\n",
    "#         time.sleep(.1)\n",
    "\n",
    "# batch_rt_values = controller.get_batch_rt_values()\n",
    "# batch_rt_values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def analyze_batch_results(batch_rt_values):\n",
    "    batch_size_results = []\n",
    "    for bs, vals in batch_rt_values.items():\n",
    "        # print(bs, len(vals), np.mean(vals))\n",
    "        batch_size_results.append({\n",
    "            'batch_size': bs,\n",
    "            'average_response_time': np.mean(vals),\n",
    "            'median_response_time': np.median(vals),\n",
    "            'p95_response_time': np.percentile(vals,95),\n",
    "        })\n",
    "\n",
    "    df_batch_size = pd.DataFrame(data=batch_size_results)\n",
    "    df_batch_size = df_batch_size.sort_values(by='batch_size')\n",
    "    return df_batch_size\n",
    "\n",
    "def plot_key_vs_batch_size(key, df_batch_size):\n",
    "    plt.figure()\n",
    "    plt.plot(df_batch_size['batch_size'], df_batch_size[key], marker='x')\n",
    "    prev_ylim = plt.gca().get_ylim()\n",
    "    # relative average response time by batch size (linear scale)\n",
    "    relative_scaled_response_time = df_batch_size['batch_size']/df_batch_size['batch_size'].iloc[0]*df_batch_size[key].iloc[0]\n",
    "    # plot the linear baseline\n",
    "    plt.plot(df_batch_size['batch_size'], relative_scaled_response_time, ls='--')\n",
    "    plt.ylim(prev_ylim)\n",
    "    plt.title(key)\n",
    "\n",
    "# df_batch_size = analyze_batch_results(batch_rt_values)\n",
    "# display(df_batch_size)\n",
    "# plot_key_vs_batch_size('average_response_time', df_batch_size)\n",
    "# plot_key_vs_batch_size('median_response_time', df_batch_size)\n",
    "# plot_key_vs_batch_size('p95_response_time', df_batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perform Experiment and Log Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# my imports\n",
    "from helpers import kube\n",
    "from helpers import workload\n",
    "from helpers import util\n",
    "from helpers import request_funcs"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using domain 192-168-23-125.nip.io\n",
      "fetching imagenet v2\n",
      "resizing images\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04574abf7a88490d8e22b73c2c17d491"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "converting to bentoml files\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b055c80d8bac4740a8e0eef8066fb61a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting base64 files\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc62d72339884e77a466a0097605f0b7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preprocessing for mobilenet\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a34d7c39cf5c4b0cb39e8432a884eb35"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "config_names = [\n",
    "    'bentoml-iris-250m-512mb',\n",
    "    'bentoml-onnx-resnet50-250m-512mb',\n",
    "    'tfserving-resnetv2-250m-512mb',\n",
    "    'tfserving-mobilenetv1-250m-512mb',\n",
    "    'bentoml-pytorch-fashionmnist-250m-512mb',\n",
    "    'bentoml-keras-toxic-comments-250m-512mb',\n",
    "]\n",
    "\n",
    "workload_configs = {}\n",
    "for exp_config_name in config_names:\n",
    "    exp_file = f\"deployments/{exp_config_name}.json\"\n",
    "    workload_spec = util.load_json_file(exp_file)\n",
    "    workload_configs[workload_spec['name']] = workload_spec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# deploy the function\n",
    "request_limit_override = f\"'cpu={cpu_m}m,memory={ram_mb}Mi'\"\n",
    "print('Request Limit Override:', request_limit_override)\n",
    "\n",
    "workload_spec = workload_configs[service_name]\n",
    "# override request and limit values\n",
    "workload_spec['opts']['--request'] = request_limit_override\n",
    "workload_spec['opts']['--limit'] = request_limit_override\n",
    "kn_command = kube.get_kn_command(**workload_spec)\n",
    "print(kn_command)\n",
    "!{kn_command}\n",
    "print('waiting for settings to converge')\n",
    "time.sleep(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Request Limit Override: 'cpu=1000m,memory=1024Mi'\n",
      "kn service apply bentoml-iris --image ghcr.io/nimamahmoudi/bentoml-iris-classifier:20210429201447 \\\n",
      "  --limit 'cpu=1000m,memory=1024Mi' \\\n",
      "  --request 'cpu=1000m,memory=1024Mi' \\\n",
      "  --port 5000 \\\n",
      "  -a autoscaling.knative.dev/target=1 \\\n",
      "  -a autoscaling.knative.dev/metric=concurrency\n",
      "No changes to apply to service 'bentoml-iris'.\n",
      "Service 'bentoml-iris' with latest revision 'bentoml-iris-00030' (unchanged) is available at URL:\n",
      "http://bentoml-iris.default.192-168-23-125.nip.io\n",
      "waiting for settings to converge\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# call the request function with proper arguments\n",
    "def call_request_func():\n",
    "    request_func = request_funcs.workload_funcs[service_name]\n",
    "    result = request_func(url=service_url)\n",
    "\n",
    "    return {\n",
    "        'response_time_ms': result['response_time_ms'],\n",
    "        'request_id': result['headers']['X-Request-Id'],\n",
    "        'queue_position': int(result['headers']['X-SmartProxy-queuePosition']),\n",
    "        'received_at': exp_trace_utils.from_js_timestamp(int(result['headers']['X-SmartProxy-receivedAt'])),\n",
    "        'response_at': exp_trace_utils.from_js_timestamp(int(result['headers']['X-SmartProxy-responseAt'])),\n",
    "        'upstream_response_time': int(result['headers']['X-SmartProxy-upstreamResponseTime']),\n",
    "        'upstream_request_count': int(result['headers']['X-SmartProxy-upstreamRequestCount']),\n",
    "        'response_time_ms_server': int(result['headers']['X-SmartProxy-responseTime']),\n",
    "        'queue_time_ms': int(result['headers']['X-SmartProxy-queueTime']),\n",
    "    }\n",
    "\n",
    "# adding exception handling to create worker func\n",
    "def worker_func():\n",
    "    try:\n",
    "        return call_request_func()\n",
    "    except Exception:\n",
    "        print('exception occured:')\n",
    "        traceback.print_exc()\n",
    "        print('Exception Time:', get_time_with_tz())\n",
    "        return None\n",
    "\n",
    "worker_func()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'response_time_ms': 2873.848,\n",
       " 'request_id': '61a02f3b-dc5d-4bdd-ac75-d0cf2f267fd4',\n",
       " 'queue_position': 0,\n",
       " 'received_at': datetime.datetime(2021, 8, 4, 13, 53, 44, 894000, tzinfo=<DstTzInfo 'America/Toronto' EDT-1 day, 20:00:00 DST>),\n",
       " 'response_at': datetime.datetime(2021, 8, 4, 13, 53, 47, 762000, tzinfo=<DstTzInfo 'America/Toronto' EDT-1 day, 20:00:00 DST>),\n",
       " 'upstream_response_time': 2807,\n",
       " 'upstream_request_count': 1,\n",
       " 'response_time_ms_server': 2868,\n",
       " 'queue_time_ms': 61}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# perform the experiment\n",
    "\n",
    "# start the controller\n",
    "if disable_controller:\n",
    "    controller.disable_controller()\n",
    "    \n",
    "controller.set_initial_config()\n",
    "controller.start_control_thread()\n",
    "\n",
    "# start workload generator\n",
    "wg = pacswg.WorkloadGenerator(worker_func=worker_func, rps=0, worker_thread_count=300)\n",
    "wg.start_workers()\n",
    "timer = TimerClass()\n",
    "\n",
    "print(\"============ Experiment Started ============\")\n",
    "print(\"Time Started:\", get_time_with_tz())\n",
    "\n",
    "for rps in tqdm(rps_list):\n",
    "    wg.set_rps(rps)\n",
    "    timer.tic()\n",
    "    # apply each for one minute\n",
    "    while timer.toc() < 60:\n",
    "        wg.fire_wait()\n",
    "\n",
    "# get the results\n",
    "wg.stop_workers()\n",
    "all_res = wg.get_stats()\n",
    "total_reqs = len(all_res)\n",
    "all_res = [d for d in all_res if d is not None]\n",
    "success_reqs = len(all_res)\n",
    "\n",
    "print(\"Total Requests Made:\", total_reqs)\n",
    "print(\"Successful Requests Made:\", success_reqs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============ Experiment Started ============\n",
      "Time Started: 2021-08-04 13:53:49.255685-04:00\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "696ab29de4dd47ae97156d6b8c2a5d84"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n",
      "Exception Time: 2021-08-04 15:09:03.281339-04:00\n",
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n",
      "Exception Time: 2021-08-04 15:09:03.283270-04:00\n",
      "exception occured:\n",
      "exception occured:\n",
      "Exception Time: 2021-08-04 15:09:03.284182-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.284367-04:00\n",
      "exception occured:\n",
      "Exception Time: 2021-08-04 15:09:03.285841-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.286062-04:00\n",
      "exception occured:\n",
      "exception occured:\n",
      "Exception Time: 2021-08-04 15:09:03.289800-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.290263-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.290499-04:00\n",
      "exception occured:\n",
      "Exception Time: 2021-08-04 15:09:03.292177-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.292493-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.292670-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.292848-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.293066-04:00\n",
      "Exception Time: Exception Time: 2021-08-04 15:09:03.293676-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.293842-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.294091-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.294260-04:00\n",
      "Exception Time: 2021-08-04 15:09:03.294412-04:00\n",
      "2021-08-04 15:09:03.293490-04:00\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-e3199f27d2a2>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 78, in request_bentoml_iris\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-iris\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Requests Made: 184154\n",
      "Successful Requests Made: 184134\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# collect the results\n",
    "df_res = pd.DataFrame(data=all_res)\n",
    "# save the results\n",
    "now = get_time_with_tz()\n",
    "res_name = now.strftime('res-%Y-%m-%d_%H-%M-%S')\n",
    "res_folder = f'results/trace_{trace_name}/{service_name}'\n",
    "# make the directory and file names\n",
    "! mkdir -p {res_folder}\n",
    "requests_results_filename = f'{res_name}_reqs.csv'\n",
    "proxy_results_filesname = f'{res_name}_proxy.csv'\n",
    "if disable_controller:\n",
    "    requests_results_filename = requests_results_filename.replace('.csv', '_no_controller.csv')\n",
    "    proxy_results_filesname = proxy_results_filesname.replace('.csv', '_no_controller.csv')\n",
    "\n",
    "df_res.to_csv(os.path.join(res_folder, requests_results_filename))\n",
    "print('Results Name:', res_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results Name: res-2021-08-04_15-54-03\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# stop the controller to save the results\n",
    "controller.stop_control_thread()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "df_proxy_stats = pd.DataFrame(data=controller.acc_proxy_stats)\n",
    "df_proxy_stats.to_csv(os.path.join(res_folder, proxy_results_filesname))\n",
    "df_proxy_stats.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   maxBufferSize  averageMaxBufferSize  averageActualBatchSize  \\\n",
       "0              5                     5                     NaN   \n",
       "1              3                     3                1.843972   \n",
       "2              8                     8                1.853659   \n",
       "3              5                     5                1.903571   \n",
       "4              3                     3                1.932331   \n",
       "\n",
       "   maxBufferTimeoutMs  currentReplicaCount  currentReadyReplicaCount  \\\n",
       "0                 160                    2                         0   \n",
       "1                 160                    2                         2   \n",
       "2                 160                    1                         1   \n",
       "3                 160                    1                         1   \n",
       "4                 160                    1                         1   \n",
       "\n",
       "   currentConcurrency  averageConcurrency  averageArrivalRate  \\\n",
       "0                   0                 0.2                0.05   \n",
       "1                   0                 0.5                9.40   \n",
       "2                   0                 0.4                8.90   \n",
       "3                   0                 0.8                8.90   \n",
       "4                   0                 0.6                8.35   \n",
       "\n",
       "   averageDepartureRate  averageDispatchRate  averageErrorRate  \\\n",
       "0                  0.00                 0.05               0.0   \n",
       "1                  9.40                 9.40               0.0   \n",
       "2                  8.90                 8.90               0.0   \n",
       "3                  8.90                 8.90               0.0   \n",
       "4                  8.45                 8.45               0.0   \n",
       "\n",
       "   averageTimeoutRatio  reponseTimeAverage  reponseTimeP50  reponseTimeP95  \\\n",
       "0             1.000000                 NaN             NaN             NaN   \n",
       "1             0.731959           85.930769            73.5           123.0   \n",
       "2             1.000000           90.894737           103.0           123.0   \n",
       "3             0.988889           94.696060           121.0           123.0   \n",
       "4             0.686047           87.036965           106.0           123.0   \n",
       "\n",
       "                              batchResponseTimeStats  \n",
       "0                                                 {}  \n",
       "1  {'1': {'values': [11, 11, 11, 11, 11, 11, 11, ...  \n",
       "2  {'1': {'values': [11, 11, 11, 11, 11, 11, 11, ...  \n",
       "3  {'1': {'values': [10, 10, 11, 11, 11, 11, 11, ...  \n",
       "4  {'1': {'values': [10, 10, 10, 11, 11, 11, 11, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxBufferSize</th>\n",
       "      <th>averageMaxBufferSize</th>\n",
       "      <th>averageActualBatchSize</th>\n",
       "      <th>maxBufferTimeoutMs</th>\n",
       "      <th>currentReplicaCount</th>\n",
       "      <th>currentReadyReplicaCount</th>\n",
       "      <th>currentConcurrency</th>\n",
       "      <th>averageConcurrency</th>\n",
       "      <th>averageArrivalRate</th>\n",
       "      <th>averageDepartureRate</th>\n",
       "      <th>averageDispatchRate</th>\n",
       "      <th>averageErrorRate</th>\n",
       "      <th>averageTimeoutRatio</th>\n",
       "      <th>reponseTimeAverage</th>\n",
       "      <th>reponseTimeP50</th>\n",
       "      <th>reponseTimeP95</th>\n",
       "      <th>batchResponseTimeStats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.843972</td>\n",
       "      <td>160</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>9.40</td>\n",
       "      <td>9.40</td>\n",
       "      <td>9.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>85.930769</td>\n",
       "      <td>73.5</td>\n",
       "      <td>123.0</td>\n",
       "      <td>{'1': {'values': [11, 11, 11, 11, 11, 11, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.853659</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.90</td>\n",
       "      <td>8.90</td>\n",
       "      <td>8.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>90.894737</td>\n",
       "      <td>103.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>{'1': {'values': [11, 11, 11, 11, 11, 11, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.903571</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8.90</td>\n",
       "      <td>8.90</td>\n",
       "      <td>8.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>94.696060</td>\n",
       "      <td>121.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>{'1': {'values': [10, 10, 11, 11, 11, 11, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.932331</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>8.35</td>\n",
       "      <td>8.45</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.686047</td>\n",
       "      <td>87.036965</td>\n",
       "      <td>106.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>{'1': {'values': [10, 10, 10, 11, 11, 11, 11, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  }
 ]
}