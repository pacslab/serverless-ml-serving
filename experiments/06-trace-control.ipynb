{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4f7b070ef049bbd8502237a868479304b4da16a010bb509c19371ab8ff01cc23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import time\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "# library imports\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pacswg.timer import TimerClass\n",
    "import pacswg\n",
    "\n",
    "from exp_trace_utils import get_time_with_tz\n",
    "import exp_trace_utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# configs\n",
    "server_address = 'http://localhost:3000'\n",
    "\n",
    "# service_name = 'tfserving-resnetv2'\n",
    "# slo_timeout = \n",
    "\n",
    "# service_name = 'bentoml-onnx-resnet50'\n",
    "# slo_timeout = \n",
    "\n",
    "# service_name = 'bentoml-iris'\n",
    "# service_url = f'{server_address}/proxy/{service_name}'\n",
    "# slo_timeout = 500\n",
    "# initial_batch_size = 5\n",
    "# bs_config = {\n",
    "#     'max_bs': 20,\n",
    "#     'min_bs': 1,\n",
    "#     'inc_step': 5,\n",
    "#     'dec_mult': 0.7,\n",
    "# }\n",
    "# average_timeout_ratio_threshold = 0.9\n",
    "# max_rps = 200\n",
    "\n",
    "# service_name = 'tfserving-mobilenetv1'\n",
    "# slo_timeout = \n",
    "\n",
    "# service_name = 'bentoml-keras-toxic-comments'\n",
    "# service_url = f'{server_address}/proxy/{service_name}'\n",
    "# slo_timeout = 500\n",
    "# initial_batch_size = 5\n",
    "# bs_config = {\n",
    "#     'max_bs': 10,\n",
    "#     'min_bs': 1,\n",
    "#     'inc_step': 5,\n",
    "#     'dec_mult': 0.7,\n",
    "# }\n",
    "# average_timeout_ratio_threshold = 0.5\n",
    "# max_rps = 50\n",
    "\n",
    "service_name = 'bentoml-pytorch-fashion-mnist'\n",
    "service_url = f'{server_address}/proxy/{service_name}'\n",
    "slo_timeout = 1000\n",
    "initial_batch_size = 5\n",
    "bs_config = {\n",
    "    'max_bs': 20,\n",
    "    'min_bs': 1,\n",
    "    'inc_step': 5,\n",
    "    'dec_mult': 0.7,\n",
    "}\n",
    "average_timeout_ratio_threshold = 0.5\n",
    "max_rps = 30\n",
    "\n",
    "# SLO Target\n",
    "slo_target = slo_timeout * 0.8\n",
    "\n",
    "# length of measurements used to estimate different batch size response times\n",
    "upstream_rt_max_len = 1000\n",
    "\n",
    "# disable controller?\n",
    "disable_controller = True\n",
    "if disable_controller:\n",
    "    initial_batch_size = 1\n",
    "\n",
    "# experiment info\n",
    "cpu_m = 1000\n",
    "ram_mb = 1024\n",
    "\n",
    "controller = exp_trace_utils.SmartProxyController(\n",
    "    server_address=server_address,\n",
    "    service_name=service_name,\n",
    "    slo_timeout=slo_timeout,\n",
    "    initial_batch_size=initial_batch_size,\n",
    "    bs_config=bs_config,\n",
    "    average_timeout_ratio_threshold=average_timeout_ratio_threshold,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# initialize trace values\n",
    "\n",
    "# staircase workload\n",
    "# base_rps = 5\n",
    "# rps_list = [base_rps] * 5 + [base_rps*2] * 5 + [base_rps*3] * 5 + [base_rps*4] * 5\n",
    "# trace_name = 'staircase'\n",
    "\n",
    "# autoscale traces\n",
    "# trace_name = 'trace_wc'\n",
    "trace_name = 'trace_t5'\n",
    "autoscale_folder_path = '../traces/files/AutoScale/'\n",
    "autoscale_trace_file_names = {\n",
    "  # Big Spike, NLANR [nlanr1995]\n",
    "  'trace_t2': 'trace_t2.txt',\n",
    "  # Dual Phase, NLANR [nlanr1995]\n",
    "  'trace_t4': 'trace_t4.txt',\n",
    "  # Large variations, NLANR [nlanr1995]\n",
    "  'trace_t5': 'trace_t5.txt',\n",
    "  # worldcup, slowly varying [ita 1998]\n",
    "  'trace_wc': 'trace_wc.txt',\n",
    "}\n",
    "autoscale_file_path = autoscale_folder_path + autoscale_trace_file_names[trace_name]\n",
    "print('loading file:', autoscale_file_path)\n",
    "trace_arr = np.loadtxt(autoscale_file_path)\n",
    "rps_list = trace_arr / (trace_arr.max()) * max_rps\n",
    "rps_list = list(rps_list)\n",
    "\n",
    "plt.plot(rps_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting and Setting Stats and Configs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "controller.set_initial_config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proxy_stats = controller.get_proxy_stats()\n",
    "proxy_stats"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "batch_rt_values = controller.update_batch_rt_values()\n",
    "# batch_rt_values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run a loop to monitor response times to test the functionality\n",
    "# timer = TimerClass()\n",
    "\n",
    "# batch_rt_values = {}\n",
    "# for _ in tqdm(range(1*6)):\n",
    "#     timer.tic()\n",
    "#     controller.update_batch_rt_values()\n",
    "#     while timer.toc() < 10:\n",
    "#         time.sleep(.1)\n",
    "\n",
    "# batch_rt_values = controller.get_batch_rt_values()\n",
    "# batch_rt_values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def analyze_batch_results(batch_rt_values):\n",
    "    batch_size_results = []\n",
    "    for bs, vals in batch_rt_values.items():\n",
    "        # print(bs, len(vals), np.mean(vals))\n",
    "        batch_size_results.append({\n",
    "            'batch_size': bs,\n",
    "            'average_response_time': np.mean(vals),\n",
    "            'median_response_time': np.median(vals),\n",
    "            'p95_response_time': np.percentile(vals,95),\n",
    "        })\n",
    "\n",
    "    df_batch_size = pd.DataFrame(data=batch_size_results)\n",
    "    df_batch_size = df_batch_size.sort_values(by='batch_size')\n",
    "    return df_batch_size\n",
    "\n",
    "def plot_key_vs_batch_size(key, df_batch_size):\n",
    "    plt.figure()\n",
    "    plt.plot(df_batch_size['batch_size'], df_batch_size[key], marker='x')\n",
    "    prev_ylim = plt.gca().get_ylim()\n",
    "    # relative average response time by batch size (linear scale)\n",
    "    relative_scaled_response_time = df_batch_size['batch_size']/df_batch_size['batch_size'].iloc[0]*df_batch_size[key].iloc[0]\n",
    "    # plot the linear baseline\n",
    "    plt.plot(df_batch_size['batch_size'], relative_scaled_response_time, ls='--')\n",
    "    plt.ylim(prev_ylim)\n",
    "    plt.title(key)\n",
    "\n",
    "# df_batch_size = analyze_batch_results(batch_rt_values)\n",
    "# display(df_batch_size)\n",
    "# plot_key_vs_batch_size('average_response_time', df_batch_size)\n",
    "# plot_key_vs_batch_size('median_response_time', df_batch_size)\n",
    "# plot_key_vs_batch_size('p95_response_time', df_batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perform Experiment and Log Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# my imports\n",
    "from helpers import kube\n",
    "from helpers import workload\n",
    "from helpers import util\n",
    "from helpers import request_funcs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config_names = [\n",
    "    'bentoml-iris-250m-512mb',\n",
    "    'bentoml-onnx-resnet50-250m-512mb',\n",
    "    'tfserving-resnetv2-250m-512mb',\n",
    "    'tfserving-mobilenetv1-250m-512mb',\n",
    "    'bentoml-pytorch-fashionmnist-250m-512mb',\n",
    "    'bentoml-keras-toxic-comments-250m-512mb',\n",
    "]\n",
    "\n",
    "workload_configs = {}\n",
    "for exp_config_name in config_names:\n",
    "    exp_file = f\"deployments/{exp_config_name}.json\"\n",
    "    workload_spec = util.load_json_file(exp_file)\n",
    "    workload_configs[workload_spec['name']] = workload_spec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# deploy the function\n",
    "request_limit_override = f\"'cpu={cpu_m}m,memory={ram_mb}Mi'\"\n",
    "print('Request Limit Override:', request_limit_override)\n",
    "\n",
    "workload_spec = workload_configs[service_name]\n",
    "# override request and limit values\n",
    "workload_spec['opts']['--request'] = request_limit_override\n",
    "workload_spec['opts']['--limit'] = request_limit_override\n",
    "kn_command = kube.get_kn_command(**workload_spec)\n",
    "print(kn_command)\n",
    "!{kn_command}\n",
    "print('waiting for settings to converge')\n",
    "time.sleep(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# call the request function with proper arguments\n",
    "def call_request_func():\n",
    "    request_func = request_funcs.workload_funcs[service_name]\n",
    "    result = request_func(url=service_url)\n",
    "\n",
    "    return {\n",
    "        'response_time_ms': result['response_time_ms'],\n",
    "        'request_id': result['headers']['X-Request-Id'],\n",
    "        'queue_position': int(result['headers']['X-SmartProxy-queuePosition']),\n",
    "        'received_at': exp_trace_utils.from_js_timestamp(int(result['headers']['X-SmartProxy-receivedAt'])),\n",
    "        'response_at': exp_trace_utils.from_js_timestamp(int(result['headers']['X-SmartProxy-responseAt'])),\n",
    "        'upstream_response_time': int(result['headers']['X-SmartProxy-upstreamResponseTime']),\n",
    "        'upstream_request_count': int(result['headers']['X-SmartProxy-upstreamRequestCount']),\n",
    "        'response_time_ms_server': int(result['headers']['X-SmartProxy-responseTime']),\n",
    "        'queue_time_ms': int(result['headers']['X-SmartProxy-queueTime']),\n",
    "    }\n",
    "\n",
    "# adding exception handling to create worker func\n",
    "def worker_func():\n",
    "    try:\n",
    "        return call_request_func()\n",
    "    except Exception:\n",
    "        print('exception occured:')\n",
    "        traceback.print_exc()\n",
    "        print('Exception Time:', get_time_with_tz())\n",
    "        return None\n",
    "\n",
    "worker_func()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# perform the experiment\n",
    "\n",
    "# start the controller\n",
    "if disable_controller:\n",
    "    controller.disable_controller()\n",
    "    \n",
    "controller.set_initial_config()\n",
    "controller.start_control_thread()\n",
    "\n",
    "# start workload generator\n",
    "wg = pacswg.WorkloadGenerator(worker_func=worker_func, rps=0, worker_thread_count=300)\n",
    "wg.start_workers()\n",
    "timer = TimerClass()\n",
    "\n",
    "print(\"============ Experiment Started ============\")\n",
    "print(\"Time Started:\", get_time_with_tz())\n",
    "\n",
    "for rps in tqdm(rps_list):\n",
    "    wg.set_rps(rps)\n",
    "    timer.tic()\n",
    "    # apply each for one minute\n",
    "    while timer.toc() < 60:\n",
    "        wg.fire_wait()\n",
    "\n",
    "# get the results\n",
    "wg.stop_workers()\n",
    "all_res = wg.get_stats()\n",
    "total_reqs = len(all_res)\n",
    "all_res = [d for d in all_res if d is not None]\n",
    "success_reqs = len(all_res)\n",
    "\n",
    "print(\"Total Requests Made:\", total_reqs)\n",
    "print(\"Successful Requests Made:\", success_reqs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# collect the results\n",
    "df_res = pd.DataFrame(data=all_res)\n",
    "# save the results\n",
    "now = get_time_with_tz()\n",
    "res_name = now.strftime('res-%Y-%m-%d_%H-%M-%S')\n",
    "res_folder = f'results/trace_{trace_name}/{service_name}'\n",
    "# make the directory and file names\n",
    "! mkdir -p {res_folder}\n",
    "requests_results_filename = f'{res_name}_reqs.csv'\n",
    "proxy_results_filesname = f'{res_name}_proxy.csv'\n",
    "if disable_controller:\n",
    "    requests_results_filename = requests_results_filename.replace('.csv', '_no_controller.csv')\n",
    "    proxy_results_filesname = proxy_results_filesname.replace('.csv', '_no_controller.csv')\n",
    "\n",
    "df_res.to_csv(os.path.join(res_folder, requests_results_filename))\n",
    "print('Results Name:', res_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# stop the controller to save the results\n",
    "controller.stop_control_thread()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_proxy_stats = pd.DataFrame(data=controller.acc_proxy_stats)\n",
    "df_proxy_stats.to_csv(os.path.join(res_folder, proxy_results_filesname))\n",
    "df_proxy_stats.head()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}