{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4f7b070ef049bbd8502237a868479304b4da16a010bb509c19371ab8ff01cc23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import time\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "# library imports\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pacswg.timer import TimerClass\n",
    "import pacswg\n",
    "\n",
    "from exp_trace_utils import get_time_with_tz\n",
    "import exp_trace_utils"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# configs\n",
    "server_address = 'http://localhost:3000'\n",
    "\n",
    "# service_name = 'tfserving-resnetv2'\n",
    "# slo_timeout = \n",
    "\n",
    "# service_name = 'bentoml-onnx-resnet50'\n",
    "# slo_timeout = \n",
    "\n",
    "# service_name = 'bentoml-iris'\n",
    "# service_url = f'{server_address}/proxy/{service_name}'\n",
    "# slo_timeout = 500\n",
    "# initial_batch_size = 5\n",
    "# bs_config = {\n",
    "#     'max_bs': 100,\n",
    "#     'min_bs': 1,\n",
    "#     'inc_step': 5,\n",
    "#     'dec_mult': 0.7,\n",
    "# }\n",
    "# average_timeout_ratio_threshold = 0.5\n",
    "\n",
    "# service_name = 'tfserving-mobilenetv1'\n",
    "# slo_timeout = \n",
    "\n",
    "service_name = 'bentoml-keras-toxic-comments'\n",
    "service_url = f'{server_address}/proxy/{service_name}'\n",
    "slo_timeout = 500\n",
    "initial_batch_size = 5\n",
    "bs_config = {\n",
    "    'max_bs': 50,\n",
    "    'min_bs': 1,\n",
    "    'inc_step': 5,\n",
    "    'dec_mult': 0.7,\n",
    "}\n",
    "average_timeout_ratio_threshold = 0.5\n",
    "\n",
    "# service_name = 'bentoml-pytorch-fashion-mnist'\n",
    "# service_url = f'{server_address}/proxy/{service_name}'\n",
    "# slo_timeout = 1000\n",
    "# initial_batch_size = 5\n",
    "# bs_config = {\n",
    "#     'max_bs': 100,\n",
    "#     'min_bs': 1,\n",
    "#     'inc_step': 5,\n",
    "#     'dec_mult': 0.7,\n",
    "# }\n",
    "# average_timeout_ratio_threshold = 0.5\n",
    "\n",
    "# SLO Target\n",
    "slo_target = slo_timeout * 0.8\n",
    "\n",
    "# length of measurements used to estimate different batch size response times\n",
    "upstream_rt_max_len = 1000\n",
    "\n",
    "# disable controller?\n",
    "disable_controller = False\n",
    "if disable_controller:\n",
    "    initial_batch_size = 1\n",
    "\n",
    "# experiment info\n",
    "cpu_m = 1000\n",
    "ram_mb = 1024\n",
    "\n",
    "controller = exp_trace_utils.SmartProxyController(\n",
    "    server_address=server_address,\n",
    "    service_name=service_name,\n",
    "    slo_timeout=slo_timeout,\n",
    "    initial_batch_size=initial_batch_size,\n",
    "    bs_config=bs_config,\n",
    "    average_timeout_ratio_threshold=average_timeout_ratio_threshold,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# initialize trace values\n",
    "\n",
    "# staircase workload\n",
    "# base_rps = 5\n",
    "# rps_list = [base_rps] * 5 + [base_rps*2] * 5 + [base_rps*3] * 5 + [base_rps*4] * 5\n",
    "# trace_name = 'staircase'\n",
    "\n",
    "# autoscale traces\n",
    "trace_name = 'trace_wc'\n",
    "max_rps = 30\n",
    "autoscale_folder_path = '../traces/files/AutoScale/'\n",
    "autoscale_trace_file_names = {\n",
    "  # Big Spike, NLANR [nlanr1995]\n",
    "  'trace_t2': 'trace_t2.txt',\n",
    "  # Dual Phase, NLANR [nlanr1995]\n",
    "  'trace_t4': 'trace_t4.txt',\n",
    "  # Large variations, NLANR [nlanr1995]\n",
    "  'trace_t5': 'trace_t5.txt',\n",
    "  # worldcup, slowly varying [ita 1998]\n",
    "  'trace_wc': 'trace_wc.txt',\n",
    "}\n",
    "autoscale_file_path = autoscale_folder_path + autoscale_trace_file_names[trace_name]\n",
    "print('loading file:', autoscale_file_path)\n",
    "trace_arr = np.loadtxt(autoscale_file_path)\n",
    "rps_list = trace_arr / (trace_arr.max()) * max_rps\n",
    "rps_list = list(rps_list)\n",
    "\n",
    "plt.plot(rps_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading file: ../traces/files/AutoScale/trace_wc.txt\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f39a37635e0>]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting and Setting Stats and Configs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "controller.set_initial_config()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'serviceName': 'bentoml-keras-toxic-comments',\n",
       " 'upstreamUrl': 'http://bentoml-keras-toxic-comments.default.192-168-23-125.nip.io/predict',\n",
       " 'maxBufferTimeoutMs': 400,\n",
       " 'maxBufferSize': 5,\n",
       " 'isTFServing': False}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "proxy_stats = controller.get_proxy_stats()\n",
    "proxy_stats"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'maxBufferSize': 5,\n",
       " 'averageMaxBufferSize': 1.4,\n",
       " 'averageActualBatchSize': None,\n",
       " 'maxBufferTimeoutMs': 400,\n",
       " 'currentReplicaCount': 0,\n",
       " 'currentReadyReplicaCount': 0,\n",
       " 'currentConcurrency': 0,\n",
       " 'averageConcurrency': 0,\n",
       " 'averageArrivalRate': 0,\n",
       " 'averageDepartureRate': 0,\n",
       " 'averageDispatchRate': 0,\n",
       " 'averageErrorRate': 0,\n",
       " 'averageTimeoutRatio': None,\n",
       " 'reponseTimeAverage': None,\n",
       " 'reponseTimeP50': None,\n",
       " 'reponseTimeP95': None,\n",
       " 'batchResponseTimeStats': {}}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "batch_rt_values = controller.update_batch_rt_values()\n",
    "# batch_rt_values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# run a loop to monitor response times to test the functionality\n",
    "# timer = TimerClass()\n",
    "\n",
    "# batch_rt_values = {}\n",
    "# for _ in tqdm(range(1*6)):\n",
    "#     timer.tic()\n",
    "#     controller.update_batch_rt_values()\n",
    "#     while timer.toc() < 10:\n",
    "#         time.sleep(.1)\n",
    "\n",
    "# batch_rt_values = controller.get_batch_rt_values()\n",
    "# batch_rt_values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def analyze_batch_results(batch_rt_values):\n",
    "    batch_size_results = []\n",
    "    for bs, vals in batch_rt_values.items():\n",
    "        # print(bs, len(vals), np.mean(vals))\n",
    "        batch_size_results.append({\n",
    "            'batch_size': bs,\n",
    "            'average_response_time': np.mean(vals),\n",
    "            'median_response_time': np.median(vals),\n",
    "            'p95_response_time': np.percentile(vals,95),\n",
    "        })\n",
    "\n",
    "    df_batch_size = pd.DataFrame(data=batch_size_results)\n",
    "    df_batch_size = df_batch_size.sort_values(by='batch_size')\n",
    "    return df_batch_size\n",
    "\n",
    "def plot_key_vs_batch_size(key, df_batch_size):\n",
    "    plt.figure()\n",
    "    plt.plot(df_batch_size['batch_size'], df_batch_size[key], marker='x')\n",
    "    prev_ylim = plt.gca().get_ylim()\n",
    "    # relative average response time by batch size (linear scale)\n",
    "    relative_scaled_response_time = df_batch_size['batch_size']/df_batch_size['batch_size'].iloc[0]*df_batch_size[key].iloc[0]\n",
    "    # plot the linear baseline\n",
    "    plt.plot(df_batch_size['batch_size'], relative_scaled_response_time, ls='--')\n",
    "    plt.ylim(prev_ylim)\n",
    "    plt.title(key)\n",
    "\n",
    "# df_batch_size = analyze_batch_results(batch_rt_values)\n",
    "# display(df_batch_size)\n",
    "# plot_key_vs_batch_size('average_response_time', df_batch_size)\n",
    "# plot_key_vs_batch_size('median_response_time', df_batch_size)\n",
    "# plot_key_vs_batch_size('p95_response_time', df_batch_size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perform Experiment and Log Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# my imports\n",
    "from helpers import kube\n",
    "from helpers import workload\n",
    "from helpers import util\n",
    "from helpers import request_funcs"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "using domain 192-168-23-125.nip.io\n",
      "fetching imagenet v2\n",
      "resizing images\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f7a2d06d9434fb690246029b15ec666"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "converting to bentoml files\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cdfa28f27134078a4213fc97bc2da59"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "extracting base64 files\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe77f1b25c714272879159b462f7715c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preprocessing for mobilenet\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ab4731afa6a4566ba5e9ff117c04879"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "config_names = [\n",
    "    'bentoml-iris-250m-512mb',\n",
    "    'bentoml-onnx-resnet50-250m-512mb',\n",
    "    'tfserving-resnetv2-250m-512mb',\n",
    "    'tfserving-mobilenetv1-250m-512mb',\n",
    "    'bentoml-pytorch-fashionmnist-250m-512mb',\n",
    "    'bentoml-keras-toxic-comments-250m-512mb',\n",
    "]\n",
    "\n",
    "workload_configs = {}\n",
    "for exp_config_name in config_names:\n",
    "    exp_file = f\"deployments/{exp_config_name}.json\"\n",
    "    workload_spec = util.load_json_file(exp_file)\n",
    "    workload_configs[workload_spec['name']] = workload_spec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# deploy the function\n",
    "request_limit_override = f\"'cpu={cpu_m}m,memory={ram_mb}Mi'\"\n",
    "print('Request Limit Override:', request_limit_override)\n",
    "\n",
    "workload_spec = workload_configs[service_name]\n",
    "# override request and limit values\n",
    "workload_spec['opts']['--request'] = request_limit_override\n",
    "workload_spec['opts']['--limit'] = request_limit_override\n",
    "kn_command = kube.get_kn_command(**workload_spec)\n",
    "print(kn_command)\n",
    "!{kn_command}\n",
    "print('waiting for settings to converge')\n",
    "time.sleep(10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Request Limit Override: 'cpu=1000m,memory=1024Mi'\n",
      "kn service apply bentoml-keras-toxic-comments --image ghcr.io/nimamahmoudi/bentoml-keras-toxic-comment-classification:20210622155420 \\\n",
      "  --limit 'cpu=1000m,memory=1024Mi' \\\n",
      "  --request 'cpu=1000m,memory=1024Mi' \\\n",
      "  --port 5000 \\\n",
      "  -a autoscaling.knative.dev/target=1 \\\n",
      "  -a autoscaling.knative.dev/metric=concurrency\n",
      "No changes to apply to service 'bentoml-keras-toxic-comments'.\n",
      "Service 'bentoml-keras-toxic-comments' with latest revision 'bentoml-keras-toxic-comments-00027' (unchanged) is available at URL:\n",
      "http://bentoml-keras-toxic-comments.default.192-168-23-125.nip.io\n",
      "waiting for settings to converge\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# call the request function with proper arguments\n",
    "def call_request_func():\n",
    "    request_func = request_funcs.workload_funcs[service_name]\n",
    "    result = request_func(url=service_url)\n",
    "\n",
    "    return {\n",
    "        'response_time_ms': result['response_time_ms'],\n",
    "        'request_id': result['headers']['X-Request-Id'],\n",
    "        'queue_position': int(result['headers']['X-SmartProxy-queuePosition']),\n",
    "        'received_at': exp_trace_utils.from_js_timestamp(int(result['headers']['X-SmartProxy-receivedAt'])),\n",
    "        'response_at': exp_trace_utils.from_js_timestamp(int(result['headers']['X-SmartProxy-responseAt'])),\n",
    "        'upstream_response_time': int(result['headers']['X-SmartProxy-upstreamResponseTime']),\n",
    "        'upstream_request_count': int(result['headers']['X-SmartProxy-upstreamRequestCount']),\n",
    "        'response_time_ms_server': int(result['headers']['X-SmartProxy-responseTime']),\n",
    "        'queue_time_ms': int(result['headers']['X-SmartProxy-queueTime']),\n",
    "    }\n",
    "\n",
    "# adding exception handling to create worker func\n",
    "def worker_func():\n",
    "    try:\n",
    "        return call_request_func()\n",
    "    except Exception:\n",
    "        print('exception occured:')\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "worker_func()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'response_time_ms': 5356.461,\n",
       " 'request_id': 'f96737dd-0d7a-4684-b12b-7baffc3551fc',\n",
       " 'queue_position': 0,\n",
       " 'received_at': datetime.datetime(2021, 7, 30, 15, 24, 21, 141000, tzinfo=<DstTzInfo 'America/Toronto' EDT-1 day, 20:00:00 DST>),\n",
       " 'response_at': datetime.datetime(2021, 7, 30, 15, 24, 26, 494000, tzinfo=<DstTzInfo 'America/Toronto' EDT-1 day, 20:00:00 DST>),\n",
       " 'upstream_response_time': 5052,\n",
       " 'upstream_request_count': 1,\n",
       " 'response_time_ms_server': 5353,\n",
       " 'queue_time_ms': 301}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# perform the experiment\n",
    "\n",
    "# start the controller\n",
    "if disable_controller:\n",
    "    controller.disable_controller()\n",
    "    \n",
    "controller.set_initial_config()\n",
    "controller.start_control_thread()\n",
    "\n",
    "# start workload generator\n",
    "wg = pacswg.WorkloadGenerator(worker_func=worker_func, rps=0, worker_thread_count=300)\n",
    "wg.start_workers()\n",
    "timer = TimerClass()\n",
    "\n",
    "print(\"============ Experiment Started ============\")\n",
    "print(\"Time Started:\", get_time_with_tz())\n",
    "\n",
    "for rps in tqdm(rps_list):\n",
    "    wg.set_rps(rps)\n",
    "    timer.tic()\n",
    "    # apply each for one minute\n",
    "    while timer.toc() < 60:\n",
    "        wg.fire_wait()\n",
    "\n",
    "# get the results\n",
    "wg.stop_workers()\n",
    "all_res = wg.get_stats()\n",
    "total_reqs = len(all_res)\n",
    "all_res = [d for d in all_res if d is not None]\n",
    "success_reqs = len(all_res)\n",
    "\n",
    "print(\"Total Requests Made:\", total_reqs)\n",
    "print(\"Successful Requests Made:\", success_reqs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============ Experiment Started ============\n",
      "Time Started: 2021-07-30 15:24:28.023549-04:00\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0423e283ae504a68b8159c746ba90ae4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exception occured:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exception occured:\n",
      "exception occured:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exception occured:\n",
      "exception occured:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exception occured:\n",
      "exception occured:\n",
      "exception occured:\n",
      "Total Requests Made: 55571\n",
      "Successful Requests Made: 55557\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 21, in worker_func\n",
      "    return call_request_func()\n",
      "  File \"<ipython-input-12-a41226ae5c73>\", line 4, in call_request_func\n",
      "    result = request_func(url=service_url)\n",
      "  File \"/home/ubuntu/serverless-ml-serving/experiments/helpers/request_funcs.py\", line 64, in request_keras_toxic_comments\n",
      "    response.raise_for_status()\n",
      "  File \"/home/ubuntu/miniconda/lib/python3.8/site-packages/requests/models.py\", line 943, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://localhost:3000/proxy/bentoml-keras-toxic-comments\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# collect the results\n",
    "df_res = pd.DataFrame(data=all_res)\n",
    "# save the results\n",
    "now = get_time_with_tz()\n",
    "res_name = now.strftime('res-%Y-%m-%d_%H-%M-%S')\n",
    "res_folder = f'results/trace_{trace_name}/{service_name}'\n",
    "# make the directory and file names\n",
    "! mkdir -p {res_folder}\n",
    "requests_results_filename = f'{res_name}_reqs.csv'\n",
    "proxy_results_filesname = f'{res_name}_proxy.csv'\n",
    "if disable_controller:\n",
    "    requests_results_filename = requests_results_filename.replace('.csv', '_no_controller.csv')\n",
    "    proxy_results_filesname = proxy_results_filesname.replace('.csv', '_no_controller.csv')\n",
    "\n",
    "df_res.to_csv(os.path.join(res_folder, requests_results_filename))\n",
    "print('Results Name:', res_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results Name: res-2021-07-30_17-38-19\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# stop the controller to save the results\n",
    "controller.stop_control_thread()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "df_proxy_stats = pd.DataFrame(data=controller.acc_proxy_stats)\n",
    "df_proxy_stats.to_csv(os.path.join(res_folder, proxy_results_filesname))\n",
    "df_proxy_stats.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   maxBufferSize  averageMaxBufferSize  averageActualBatchSize  \\\n",
       "0              5                     5                     NaN   \n",
       "1              3                     3                1.850000   \n",
       "2              2                     2                1.709677   \n",
       "3              1                     1                1.259843   \n",
       "4              6                     6                1.316667   \n",
       "\n",
       "   maxBufferTimeoutMs  currentReplicaCount  currentReadyReplicaCount  \\\n",
       "0                 400                    2                         1   \n",
       "1                 400                    2                         2   \n",
       "2                 400                    2                         2   \n",
       "3                 400                    1                         1   \n",
       "4                 400                    1                         1   \n",
       "\n",
       "   currentConcurrency  averageConcurrency  averageArrivalRate  \\\n",
       "0                   0                 0.3                0.05   \n",
       "1                   4                 3.8                2.80   \n",
       "2                   4                 3.2                2.95   \n",
       "3                   3                 3.0                2.50   \n",
       "4                   6                 3.9                2.50   \n",
       "\n",
       "   averageDepartureRate  averageDispatchRate  averageErrorRate  \\\n",
       "0                  0.00                 0.05               0.0   \n",
       "1                  2.80                 2.80               0.0   \n",
       "2                  2.95                 2.95               0.0   \n",
       "3                  2.50                 2.50               0.0   \n",
       "4                  2.55                 2.55               0.0   \n",
       "\n",
       "   averageTimeoutRatio  reponseTimeAverage  reponseTimeP50  reponseTimeP95  \\\n",
       "0             1.000000                 NaN             NaN             NaN   \n",
       "1             0.750000          336.513514           341.0           403.0   \n",
       "2             0.361111          262.823899           252.0           401.0   \n",
       "3             0.000000          135.518750            58.0           393.0   \n",
       "4             1.000000          184.816456            86.0           402.6   \n",
       "\n",
       "                              batchResponseTimeStats  \n",
       "0                                                 {}  \n",
       "1  {'1': {'values': [46, 46, 46, 47, 47, 47, 48, ...  \n",
       "2  {'1': {'values': [43, 43, 45, 45, 46, 46, 46, ...  \n",
       "3  {'1': {'values': [43, 43, 45, 45, 46, 46, 46, ...  \n",
       "4  {'1': {'values': [45, 46, 46, 46, 46, 47, 47, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maxBufferSize</th>\n",
       "      <th>averageMaxBufferSize</th>\n",
       "      <th>averageActualBatchSize</th>\n",
       "      <th>maxBufferTimeoutMs</th>\n",
       "      <th>currentReplicaCount</th>\n",
       "      <th>currentReadyReplicaCount</th>\n",
       "      <th>currentConcurrency</th>\n",
       "      <th>averageConcurrency</th>\n",
       "      <th>averageArrivalRate</th>\n",
       "      <th>averageDepartureRate</th>\n",
       "      <th>averageDispatchRate</th>\n",
       "      <th>averageErrorRate</th>\n",
       "      <th>averageTimeoutRatio</th>\n",
       "      <th>reponseTimeAverage</th>\n",
       "      <th>reponseTimeP50</th>\n",
       "      <th>reponseTimeP95</th>\n",
       "      <th>batchResponseTimeStats</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>336.513514</td>\n",
       "      <td>341.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>{'1': {'values': [46, 46, 46, 47, 47, 47, 48, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.709677</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>262.823899</td>\n",
       "      <td>252.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>{'1': {'values': [43, 43, 45, 45, 46, 46, 46, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.259843</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>135.518750</td>\n",
       "      <td>58.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>{'1': {'values': [43, 43, 45, 45, 46, 46, 46, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.316667</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>184.816456</td>\n",
       "      <td>86.0</td>\n",
       "      <td>402.6</td>\n",
       "      <td>{'1': {'values': [45, 46, 46, 46, 46, 47, 47, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  }
 ]
}